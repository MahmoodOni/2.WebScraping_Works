{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3599f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "def extract_text(tag):\n",
    "    return tag.text.strip() if tag else ''\n",
    "\n",
    "def scrape_accessories(url):\n",
    "    \"\"\"Scrapes text from div elements with class 'acctitel noLeaf' from a webpage using Selenium.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL of the webpage to scrape.\n",
    "    \n",
    "    Returns:\n",
    "        A list of strings containing the scraped text.\n",
    "    \"\"\"\n",
    "    # Initialize the Chrome driver\n",
    "    driver = webdriver.Chrome()\n",
    "    accessories_text = []\n",
    "    \n",
    "    try:\n",
    "        # Open the webpage\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Execute JavaScript to get the text of elements with class 'acctitel noLeaf'\n",
    "        accessories_elements = driver.execute_script(\n",
    "            \"return [...document.querySelectorAll('div.acctitel.noLeaf')].map(e => e.innerText.trim());\"\n",
    "        )\n",
    "        \n",
    "        accessories_text.extend(accessories_elements)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered during scraping: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "        \n",
    "    return accessories_text\n",
    "\n",
    "def scrape_child_urls(parent_url):\n",
    "    base_url = 'https://www.imm-cologne.com'  # Base URL of the website\n",
    "    response = requests.get(parent_url)\n",
    "    child_urls = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all div elements with class 'col col1ergebnis'\n",
    "        div_elements = soup.find_all('div', class_='col col1ergebnis')\n",
    "        # Extract the href attribute from the 'a' tag within each div element\n",
    "        child_urls = [base_url + div.find('a')['href'] for div in div_elements if div.find('a')]\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "    \n",
    "    return child_urls\n",
    "\n",
    "\n",
    "def scrape_data_from_child_url(child_url):\n",
    "    response = requests.get(child_url)\n",
    "    data = {}\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        company_name_div = soup.find('div', class_='headline-title')\n",
    "        company_name = extract_text(company_name_div.find('span')) if company_name_div else ''\n",
    "\n",
    "#         company_address_div = soup.find('div', class_='location-info')\n",
    "#         if company_address_div:\n",
    "#             address_components = [element.strip() for element in company_address_div.stripped_strings]\n",
    "#             company_address = ', '.join(address_components)\n",
    "#         else:\n",
    "#             company_address = ''\n",
    "\n",
    "        company_address_div = soup.find('div', class_='location-info')\n",
    "        if company_address_div:\n",
    "            address_components = [element.strip() for element in company_address_div.stripped_strings]\n",
    "            company_address = ', '.join(address_components)\n",
    "            words = company_address.split()\n",
    "            location = words[-1] if words else ''\n",
    "        else:\n",
    "            company_address = ''\n",
    "            location = ''\n",
    "\n",
    "        company_booth_div = soup.find('div', class_='asdb54-rawTextHallenStand')\n",
    "        company_booth = extract_text(company_booth_div.find('div')).replace('\\n', '').replace('\\t', '') if company_booth_div else ''\n",
    "\n",
    "        company_website_div = soup.find('div', class_='sico ico_link linkellipsis')\n",
    "        company_website = extract_text(company_website_div.find('span')) if company_website_div else ''\n",
    "\n",
    "        company_email_div = soup.find('div', class_='sico ico_email')\n",
    "        company_email = extract_text(company_email_div.find('span')) if company_email_div else ''\n",
    "\n",
    "        company_phone_div = soup.find('div', class_='sico ico_phone')\n",
    "        company_phone = extract_text(company_phone_div)\n",
    "\n",
    "        scraped_accessories = scrape_accessories(child_url)\n",
    "\n",
    "        targeted_countries = [extract_text(div) for div in soup.find_all('div', class_='asdb54-singleInfo-gruppierung')]\n",
    "        targeted_countries_filtered = targeted_countries[2:-1]\n",
    "\n",
    "\n",
    "        data = {\n",
    "            'Exhibitor Name': [company_name],\n",
    "            'Exhibitor Address': [company_address],\n",
    "            'Exhibitor Booth': [company_booth],\n",
    "            'Exhibitor Website': [company_website],\n",
    "            'Exhibitor Email': [company_email],\n",
    "            'Exhibitor Phone': [company_phone],\n",
    "            'Product Groups': [scraped_accessories],\n",
    "            'Targeted Countries': [targeted_countries_filtered],\n",
    "            'Location' : location\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def main():\n",
    "    parent_url = 'https://www.imm-cologne.com/imm-cologne-exhibitors/list-of-exhibitors/'\n",
    "    child_urls = []\n",
    "\n",
    "    for page_number in range(1, 38):\n",
    "        start_value = (page_number - 1) * 20\n",
    "        #page_url = f\"{parent_url}?route=aussteller/blaettern&&start={page_number * 15}&paginatevalues=%7B%22stichwort%22%3A%22%22%2C%22suchart%22%3A%22alle%22%7D\"\n",
    "        page_url = f\"{parent_url}?route=aussteller/blaettern&&start={start_value}&paginatevalues=%7B%22stichwort%22%3A%22%22%2C%22suchart%22%3A%22alle%22%7D\"\n",
    "        child_urls.extend(scrape_child_urls(page_url))\n",
    "    \n",
    "    all_data = []\n",
    "    for child_url in child_urls:\n",
    "        data = scrape_data_from_child_url(child_url)\n",
    "        all_data.append(data)\n",
    "    \n",
    "    df = pd.concat([pd.DataFrame(data) for data in all_data], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "result_df = main()\n",
    "#print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f81bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
